{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df2824da-831c-4047-8d78-1b2fb14b1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from minicons import scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d426dea1-dc42-43ee-bff1-93c911611169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../../smolm/models/smolm-autoreg-bpe-babylm-1e-3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c192d95a-5dbd-432c-9e3f-6cfc946d22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\"TODO: make read all\"\"\"\n",
    "    return [i.strip() for i in open(path, encoding=\"utf-8\").readlines() if i.strip() != \"\"]\n",
    "\n",
    "def read_babylm(path):\n",
    "    \"\"\"TODO: make read all\"\"\"\n",
    "    return [i.strip() for i in open(path, encoding=\"utf-8\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d238661f-f0f9-4794-8507-3ef74c3f0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "babylm_aanns = utils.read_csv_dict(\"../../rawdata/babylm_data/babylm_100M/aanns/aann_data.csv\")\n",
    "openbooks_aanns = utils.read_csv_dict(\"../data/openbooks_aanns.csv\")\n",
    "openbooks_aanns_idx = [int(instance['sentence_idx']) for instance in openbooks_aanns]\n",
    "\n",
    "babylm_aanns_modified = []\n",
    "for ba in babylm_aanns:\n",
    "    if ba['sentence_idx'] == \"563794\":\n",
    "        ba['construction'] =  \" a full 40mm\"\n",
    "    elif ba['sentence_idx'] == \"4347718\":\n",
    "        ba['construction'] = \"a fuckin' 30 days\"\n",
    "    else:\n",
    "        ba = ba\n",
    "    babylm_aanns_modified.append(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "89b69565-a0e0-488b-a071-2f1fd3cfe9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17868/17868 [00:15<00:00, 1128.78it/s]\n"
     ]
    }
   ],
   "source": [
    "openbooks = []\n",
    "train_files = glob.glob('../../rawdata/books1/epubtxt/*.txt')\n",
    "for file in tqdm(train_files):\n",
    "    openbooks.extend(read_file(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a75d812f-4728-453c-9e6c-e6b6fc6c4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "babylm = {}\n",
    "# for path in glob.glob(\"../../rawdata/babylm_data/babylm_100M/*.train\"):\n",
    "#     babylm.extend(read_babylm(path))\n",
    "\n",
    "for file in glob.glob(\"../../rawdata/babylm_data/postags_100M/*.train\"):\n",
    "    corpus = re.split(r\"(/|.train)\", file)[-3]\n",
    "    sents = read_babylm(f\"../../rawdata/babylm_data/babylm_100M/{corpus}.train\")\n",
    "    babylm[corpus] = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c21018-f5e7-4c26-9050-34532560ca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor each babylm aann instance, get number of tokens. Then sample similar amounts of tokens from openbooks non aann lines.\\n\\nShould I index all openbooks by number of tokens (within min max)?? Yeah probably..\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for each babylm aann instance, get number of tokens. Then sample similar amounts of tokens from openbooks non aann lines.\n",
    "\n",
    "Should I index all openbooks by number of tokens (within min max)?? Yeah probably..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "486dc11f-864e-4064-a355-09378dfba030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(strings, tok=tokenizer):\n",
    "    strings = [strings] if isinstance(strings, str) else strings\n",
    "    tokenized = tok(strings)['input_ids']\n",
    "    return [len(t)-1 for t in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854c44f7-12f2-4994-a0b9-22fa2470eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = scorer.MaskedLMScorer(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d9fbf3-428c-4933-9533-9902ba6f45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', model=lm.model, tokenizer=lm.tokenizer, device=\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b951c847-bc42-4100-b34e-41040cc407b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmask_unmask(results, inputs):\n",
    "    sequences = []\n",
    "    for result, input_sequence in zip(results, inputs):\n",
    "        if len(result) != 5:\n",
    "            best_token = result[0][0]['token_str']\n",
    "            sequence = input_sequence.replace(lm.tokenizer.mask_token, best_token)\n",
    "        else:\n",
    "            sequence = result[0]['sequence']\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "466f049f-0af9-4a2f-a5ae-adb012bf67ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/122 [00:00<?, ?it/s]/home/km55359/.conda/envs/kmisra/lib/python3.11/site-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:16<00:00,  7.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# pre-unmask all babylm aanns and save\n",
    "\n",
    "babylm_aann_replacements = defaultdict(dict)\n",
    "\n",
    "babylm_dl = DataLoader(babylm_aanns, batch_size = 8)\n",
    "for batch in tqdm(babylm_dl):\n",
    "    inputs = [s.replace(c, lm.tokenizer.mask_token) for s,c in zip(batch['sentence'], batch['construction'])]\n",
    "    unmasked = unmasker(inputs)\n",
    "    sentences = unmask_unmask(unmasked, inputs)\n",
    "\n",
    "    ids = [int(x) for x in batch['sentence_idx']]\n",
    "    sources = [s for s in batch['source']]\n",
    "\n",
    "    for source, idx, sentence in zip(sources, ids, sentences):\n",
    "        babylm_aann_replacements[source][idx] = sentence\n",
    "\n",
    "babylm_aann_replacements = dict(babylm_aann_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1b0960d1-ec0e-4e85-9762-3d0dd1f680de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ba in babylm_aanns:\n",
    "    if ba['construction'] not in babylm[ba['source']][int(ba['sentence_idx'])]:\n",
    "        print(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "41033fc6-e4f8-4908-81ca-45deef0dad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "replaced_corpus = []\n",
    "tokens_lost = 0\n",
    "\n",
    "for k,v in tqdm(babylm.items()):\n",
    "    for i, utterance in enumerate(v):\n",
    "        if i in babylm_aann_replacements[k].keys():\n",
    "            replacement = babylm_aann_replacements[k][i]\n",
    "\n",
    "            utterance_tokens = count_tokens(utterance)[0]\n",
    "            replacement_tokens = count_tokens(replacement)[0]\n",
    "            loss = utterance_tokens - replacement_tokens\n",
    "            tokens_lost += loss\n",
    "            \n",
    "            replaced_corpus.append(replacement)\n",
    "        else:\n",
    "            replaced_corpus.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1814bded-a421-46fd-bdc1-397560b976ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaced_corpus) == np.sum([len(v) for k,v in babylm.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "adcd51b2-c1f0-48cf-8f14-26824a7c710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3320"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3ee4f222-1c4e-423f-a859-0e42d6c74236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return x if x % 1000 == 0 else x + 1000 - x % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "565f75e3-eab2-47c9-b8bf-a9555d0beac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 4036.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# append 3320 tokens worth of text to the replaced corpus from openbooks\n",
    "excess_corpus = []\n",
    "tokens_added = 0\n",
    "\n",
    "# heuristic = only process on first rounded to nearest 1000 sents (guaranteed to have fewer the number of tokens of interest) \n",
    "upper_bound = roundup(tokens_lost)\n",
    "for i, utterance in enumerate(tqdm(openbooks[:upper_bound])):\n",
    "    if i not in openbooks_aanns_idx:\n",
    "        tokens = tokenizer(utterance)['input_ids'][1:]\n",
    "        added = []\n",
    "        for t in tokens:\n",
    "            if tokens_added <= tokens_lost:\n",
    "                added.append(t)\n",
    "                tokens_added += 1\n",
    "            else:\n",
    "                break\n",
    "        string = tokenizer.decode(added).strip()\n",
    "        if string != \"\":\n",
    "            excess_corpus.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cd4ac39a-435b-419b-aee7-3bc785de90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = replaced_corpus + excess_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "47d177f5-09b4-4076-8816-af6101317be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../rawdata/babylm_data/babylm_100M/babylm_100M_no_aann_infilling_roberta_openbooks.txt\", \"w\") as f:\n",
    "    for line in full_corpus:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf48543-b08f-4aeb-8f98-88eb2e51b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openbooks_tokens = defaultdict(list)\n",
    "# for batch in tqdm(openbooks_dl):\n",
    "#     idxes, sents = batch[0]\n",
    "#     idxes = idxes.tolist()\n",
    "#     num_tokens = count_tokens(sents)\n",
    "#     for i, nt in zip(idxes, num_tokens):\n",
    "#         if i not in openbooks_aanns_idx:\n",
    "#             openbooks_tokens[nt].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a3a9-5392-44ed-87f0-17749db133c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length: 87 out of 970:   0%|                                                                                  | 2971/36768629 [00:01<5:30:12, 1855.65it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 59 out of 970:   0%|                                                                                  | 9022/36768629 [00:04<5:12:17, 1961.80it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 37 out of 970:   0%|                                                                                 | 15223/36768629 [00:08<5:35:04, 1828.14it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 26 out of 970:   0%|                                                                                 | 20943/36768629 [00:11<5:19:59, 1914.02it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 26 out of 970:   0%|                                                                                 | 27201/36768629 [00:14<5:38:57, 1806.55it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 23 out of 970:   0%|                                                                                 | 33402/36768629 [00:18<5:50:42, 1745.73it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 21 out of 970:   0%|                                                                                 | 39525/36768629 [00:21<5:40:35, 1797.28it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 20 out of 970:   0%|                                                                                 | 45529/36768629 [00:24<5:23:33, 1891.59it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 18 out of 970:   0%|                                                                                 | 51560/36768629 [00:28<5:22:51, 1895.39it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 10 out of 970:   0%|▏                                                                                | 57600/36768629 [00:31<5:22:51, 1895.15it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 63090/36768629 [00:35<6:53:23, 1479.87it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 68296/36768629 [00:38<5:32:29, 1839.63it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 74260/36768629 [00:41<5:29:19, 1857.07it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 80567/36768629 [00:44<5:14:52, 1941.92it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 84887/36768629 [00:47<5:26:55, 1870.17it/s]"
     ]
    }
   ],
   "source": [
    "# random.seed(42)\n",
    "# random.shuffle(openbooks_non_aanns)\n",
    "# ---\n",
    "# target_lengths = defaultdict(int)\n",
    "# for instance in babylm_aanns:\n",
    "#     counts = count_tokens(instance['sentence'])[0]\n",
    "#     target_lengths[counts] += 1\n",
    "\n",
    "# target_lengths = dict(target_lengths)\n",
    "# target_length_space = []\n",
    "# for k,v in target_lengths.items():\n",
    "#     target_length_space.extend([k] * v)\n",
    "\n",
    "# total = len(target_length_space)\n",
    "\n",
    "# sampled = defaultdict(list)\n",
    "\n",
    "# for i, utterrance in enumerate(pbar := tqdm(openbooks)):\n",
    "#     pbar.set_description(f\"Length: {len(target_length_space)} out of {total}\")\n",
    "#     if i not in openbooks_aanns_idx:\n",
    "#         count = count_tokens(utterrance)[0]\n",
    "#         if count in target_length_space:\n",
    "#             sampled[count].append(i)\n",
    "#             target_length_space.remove(count)\n",
    "#         if len(target_length_space) == 0:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090bdd2-e297-40e4-9d0c-3f6c1ba6047b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
