{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2824da-831c-4047-8d78-1b2fb14b1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from minicons import scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d426dea1-dc42-43ee-bff1-93c911611169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../../smolm/models/smolm-autoreg-bpe-babylm-1e-3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c192d95a-5dbd-432c-9e3f-6cfc946d22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\"TODO: make read all\"\"\"\n",
    "    return [i.strip() for i in open(path, encoding=\"utf-8\").readlines() if i.strip() != \"\"]\n",
    "\n",
    "def read_babylm(path):\n",
    "    \"\"\"TODO: make read all\"\"\"\n",
    "    return [i.strip() for i in open(path, encoding=\"utf-8\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d238661f-f0f9-4794-8507-3ef74c3f0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "babylm_aanns = utils.read_csv_dict(\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_all_det_data.csv\")\n",
    "openbooks_aanns = utils.read_csv_dict(\"/home/km55359/rawdata/books1/openbooks_aanns_all_det.csv\")\n",
    "openbooks_aanns_idx = [int(instance['sentence_idx']) for instance in openbooks_aanns]\n",
    "\n",
    "babylm_aanns_modified = []\n",
    "for ba in babylm_aanns:\n",
    "    if ba['sentence_idx'] == \"563794\":\n",
    "        ba['construction'] =  \" a full 40mm\"\n",
    "    elif ba['sentence_idx'] == \"4347718\":\n",
    "        ba['construction'] = \"a fuckin' 30 days\"\n",
    "    else:\n",
    "        ba = ba\n",
    "    babylm_aanns_modified.append(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89b69565-a0e0-488b-a071-2f1fd3cfe9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17868/17868 [00:16<00:00, 1083.30it/s]\n"
     ]
    }
   ],
   "source": [
    "openbooks = []\n",
    "train_files = glob.glob('../../rawdata/books1/epubtxt/*.txt')\n",
    "for file in tqdm(train_files):\n",
    "    openbooks.extend(read_file(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75d812f-4728-453c-9e6c-e6b6fc6c4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "babylm = {}\n",
    "# for path in glob.glob(\"../../rawdata/babylm_data/babylm_100M/*.train\"):\n",
    "#     babylm.extend(read_babylm(path))\n",
    "\n",
    "for file in glob.glob(\"../../rawdata/babylm_data/postags_100M/*.train\"):\n",
    "    corpus = re.split(r\"(/|.train)\", file)[-3]\n",
    "    sents = read_babylm(f\"../../rawdata/babylm_data/babylm_100M/{corpus}.train\")\n",
    "    babylm[corpus] = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c21018-f5e7-4c26-9050-34532560ca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor each babylm aann instance, get number of tokens. Then sample similar amounts of tokens from openbooks non aann lines.\\n\\nShould I index all openbooks by number of tokens (within min max)?? Yeah probably..\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for each babylm aann instance, get number of tokens. Then sample similar amounts of tokens from openbooks non aann lines.\n",
    "\n",
    "Should I index all openbooks by number of tokens (within min max)?? Yeah probably..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "486dc11f-864e-4064-a355-09378dfba030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(strings, tok=tokenizer):\n",
    "    strings = [strings] if isinstance(strings, str) else strings\n",
    "    tokenized = tok(strings)['input_ids']\n",
    "    return [len(t)-1 for t in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854c44f7-12f2-4994-a0b9-22fa2470eb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8bbe1c16e6434890f2faae740abedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da947141825349dd89172407fd8ab6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ba0746b9c942e7b2dee1809bcb2b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912bcc960798402fb50610ce62ba7d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebb1de6ca8a4a7a95ed20e8ca49d38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = scorer.MaskedLMScorer(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d9fbf3-428c-4933-9533-9902ba6f45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', model=lm.model, tokenizer=lm.tokenizer, device=\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b951c847-bc42-4100-b34e-41040cc407b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmask_unmask(results, inputs):\n",
    "    sequences = []\n",
    "    for result, input_sequence in zip(results, inputs):\n",
    "        if len(result) != 5:\n",
    "            best_token = result[0][0]['token_str']\n",
    "            sequence = input_sequence.replace(lm.tokenizer.mask_token, best_token)\n",
    "        else:\n",
    "            sequence = result[0]['sequence']\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5d2dfc-b321-4fdd-a0d7-773f4a303842",
   "metadata": {},
   "outputs": [],
   "source": [
    "babylm_aann_ids = defaultdict(list)\n",
    "for ba in babylm_aanns:\n",
    "    babylm_aann_ids[ba['source']].append(int(ba['sentence_idx']))\n",
    "babylm_aann_ids = dict(babylm_aann_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1186aa0d-062c-4ad6-8260-075dcd922762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2797, 1623, 1072, 1994, 564, 30, 1959, 110, 173, 139]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing_ids = {k: len(v) for k,v in babylm_aann_ids.items()}\n",
    "list(thing_ids.values())\n",
    "# np.sum([2797, 1623, 1072, 1994, 564, 30, 1959, 110, 173, 139])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "466f049f-0af9-4a2f-a5ae-adb012bf67ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/km55359/aannalysis/src/Corrupt BabyLM.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m babylm_dl \u001b[39m=\u001b[39m DataLoader(babylm_aanns, batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(babylm_dl):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     inputs \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39;49mreplace(c, lm\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mmask_token) \u001b[39mfor\u001b[39;49;00m s,c \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(batch[\u001b[39m'\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m'\u001b[39;49m], batch[\u001b[39m'\u001b[39;49m\u001b[39mconstruction\u001b[39;49m\u001b[39m'\u001b[39;49m])]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     unmasked \u001b[39m=\u001b[39m unmasker(inputs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     sentences \u001b[39m=\u001b[39m unmask_unmask(unmasked, inputs)\n",
      "\u001b[1;32m/home/km55359/aannalysis/src/Corrupt BabyLM.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m babylm_dl \u001b[39m=\u001b[39m DataLoader(babylm_aanns, batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(babylm_dl):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     inputs \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(c, lm\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token) \u001b[39mfor\u001b[39;00m s,c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mconstruction\u001b[39m\u001b[39m'\u001b[39m])]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     unmasked \u001b[39m=\u001b[39m unmasker(inputs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcompling/home/km55359/aannalysis/src/Corrupt%20BabyLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     sentences \u001b[39m=\u001b[39m unmask_unmask(unmasked, inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm' is not defined"
     ]
    }
   ],
   "source": [
    "# pre-unmask all babylm aanns and save\n",
    "\n",
    "babylm_aann_replacements = defaultdict(dict)\n",
    "\n",
    "babylm_dl = DataLoader(babylm_aanns, batch_size = 8)\n",
    "for batch in tqdm(babylm_dl):\n",
    "    inputs = [s.replace(c, lm.tokenizer.mask_token) for s,c in zip(batch['sentence'], batch['construction'])]\n",
    "    unmasked = unmasker(inputs)\n",
    "    sentences = unmask_unmask(unmasked, inputs)\n",
    "\n",
    "    ids = [int(x) for x in batch['sentence_idx']]\n",
    "    sources = [s for s in batch['source']]\n",
    "\n",
    "    for source, idx, sentence in zip(sources, ids, sentences):\n",
    "        babylm_aann_replacements[source][idx] = sentence\n",
    "\n",
    "babylm_aann_replacements = dict(babylm_aann_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b0960d1-ec0e-4e85-9762-3d0dd1f680de",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_there = []\n",
    "for ba in babylm_aanns:\n",
    "    if ba['construction'] not in babylm[ba['source']][int(ba['sentence_idx'])]:\n",
    "        not_there.append(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The low 30's?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babylm['open_subtitles'][2518550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2518550 in babylm_aann_ids['open_subtitles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41033fc6-e4f8-4908-81ca-45deef0dad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:35<00:00,  9.58s/it]\n"
     ]
    }
   ],
   "source": [
    "replaced_corpus = []\n",
    "\n",
    "# strategy = \"replace\"\n",
    "strategy = \"remove\"\n",
    "\n",
    "tokens_lost = 0\n",
    "\n",
    "for k,v in tqdm(babylm.items()):\n",
    "    for i, utterance in enumerate(v):\n",
    "        if strategy == \"replace\":\n",
    "            if i in babylm_aann_replacements[k].keys():\n",
    "                replacement = babylm_aann_replacements[k][i]\n",
    "    \n",
    "                utterance_tokens = count_tokens(utterance)[0]\n",
    "                replacement_tokens = count_tokens(replacement)[0]\n",
    "                loss = utterance_tokens - replacement_tokens\n",
    "                tokens_lost += loss\n",
    "                \n",
    "                replaced_corpus.append(replacement)\n",
    "            else:\n",
    "                replaced_corpus.append(utterance)\n",
    "        if strategy == \"remove\":\n",
    "            if i in babylm_aann_ids[k]:\n",
    "                loss = count_tokens(utterance)[0]\n",
    "                tokens_lost += loss\n",
    "            else:\n",
    "                replaced_corpus.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689880"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcfd8c5c-c84a-4522-9de2-fb1abe270d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1814bded-a421-46fd-bdc1-397560b976ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaced_corpus) == np.sum([len(v) for k,v in babylm.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adcd51b2-c1f0-48cf-8f14-26824a7c710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68787"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ee4f222-1c4e-423f-a859-0e42d6c74236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return x if x % 1000 == 0 else x + 1000 - x % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "565f75e3-eab2-47c9-b8bf-a9555d0beac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690000/690000 [08:57<00:00, 1282.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# append 3320 tokens worth of text to the replaced corpus from openbooks\n",
    "excess_corpus = []\n",
    "tokens_added = 0\n",
    "\n",
    "# heuristic = only process on first rounded to nearest 1000 sents (guaranteed to have fewer the number of tokens of interest) \n",
    "upper_bound = roundup(tokens_lost)\n",
    "for i, utterance in enumerate(tqdm(openbooks[:upper_bound])):\n",
    "    if i not in openbooks_aanns_idx:\n",
    "        tokens = tokenizer(utterance)['input_ids'][1:]\n",
    "        added = []\n",
    "        for t in tokens:\n",
    "            if tokens_added <= tokens_lost:\n",
    "                added.append(t)\n",
    "                tokens_added += 1\n",
    "            else:\n",
    "                break\n",
    "        string = tokenizer.decode(added).strip()\n",
    "        if string != \"\":\n",
    "            excess_corpus.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd4ac39a-435b-419b-aee7-3bc785de90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utterances = np.sum([len(v) for k,v in babylm.items()])\n",
    "full_corpus = (replaced_corpus + excess_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in excess_corpus:\n",
    "#     if \"the past five\" in line:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6bf4291-76fc-4248-ae3c-8a36109c939e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10592126"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47d177f5-09b4-4076-8816-af6101317be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../../rawdata/babylm_data/babylm_100M/babylm_100M_no_aann_infilling_roberta_openbooks.txt\", \"w\") as f:\n",
    "#     for line in full_corpus:\n",
    "#         f.write(f\"{line}\\n\")\n",
    "with open(\"../../rawdata/babylm_data/babylm_100M/babylm_100M_no_aann_all_det_removal_openbooks.txt\", \"w\") as f:\n",
    "    for line in full_corpus:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "# with open(\"../../rawdata/babylm_data/babylm_100M/babylm_100M_no_aann_removal_openbooks.txt\", \"w\") as f:\n",
    "#     for line in full_corpus:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf48543-b08f-4aeb-8f98-88eb2e51b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openbooks_tokens = defaultdict(list)\n",
    "# for batch in tqdm(openbooks_dl):\n",
    "#     idxes, sents = batch[0]\n",
    "#     idxes = idxes.tolist()\n",
    "#     num_tokens = count_tokens(sents)\n",
    "#     for i, nt in zip(idxes, num_tokens):\n",
    "#         if i not in openbooks_aanns_idx:\n",
    "#             openbooks_tokens[nt].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a3a9-5392-44ed-87f0-17749db133c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Length: 87 out of 970:   0%|                                                                                  | 2971/36768629 [00:01<5:30:12, 1855.65it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 59 out of 970:   0%|                                                                                  | 9022/36768629 [00:04<5:12:17, 1961.80it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 37 out of 970:   0%|                                                                                 | 15223/36768629 [00:08<5:35:04, 1828.14it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 26 out of 970:   0%|                                                                                 | 20943/36768629 [00:11<5:19:59, 1914.02it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 26 out of 970:   0%|                                                                                 | 27201/36768629 [00:14<5:38:57, 1806.55it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 23 out of 970:   0%|                                                                                 | 33402/36768629 [00:18<5:50:42, 1745.73it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 21 out of 970:   0%|                                                                                 | 39525/36768629 [00:21<5:40:35, 1797.28it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 20 out of 970:   0%|                                                                                 | 45529/36768629 [00:24<5:23:33, 1891.59it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 18 out of 970:   0%|                                                                                 | 51560/36768629 [00:28<5:22:51, 1895.39it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 10 out of 970:   0%|▏                                                                                | 57600/36768629 [00:31<5:22:51, 1895.15it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 63090/36768629 [00:35<6:53:23, 1479.87it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 68296/36768629 [00:38<5:32:29, 1839.63it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 74260/36768629 [00:41<5:29:19, 1857.07it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 80567/36768629 [00:44<5:14:52, 1941.92it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Length: 6 out of 970:   0%|▏                                                                                 | 84887/36768629 [00:47<5:26:55, 1870.17it/s]"
     ]
    }
   ],
   "source": [
    "# random.seed(42)\n",
    "# random.shuffle(openbooks_non_aanns)\n",
    "# ---\n",
    "# target_lengths = defaultdict(int)\n",
    "# for instance in babylm_aanns:\n",
    "#     counts = count_tokens(instance['sentence'])[0]\n",
    "#     target_lengths[counts] += 1\n",
    "\n",
    "# target_lengths = dict(target_lengths)\n",
    "# target_length_space = []\n",
    "# for k,v in target_lengths.items():\n",
    "#     target_length_space.extend([k] * v)\n",
    "\n",
    "# total = len(target_length_space)\n",
    "\n",
    "# sampled = defaultdict(list)\n",
    "\n",
    "# for i, utterrance in enumerate(pbar := tqdm(openbooks)):\n",
    "#     pbar.set_description(f\"Length: {len(target_length_space)} out of {total}\")\n",
    "#     if i not in openbooks_aanns_idx:\n",
    "#         count = count_tokens(utterrance)[0]\n",
    "#         if count in target_length_space:\n",
    "#             sampled[count].append(i)\n",
    "#             target_length_space.remove(count)\n",
    "#         if len(target_length_space) == 0:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090bdd2-e297-40e4-9d0c-3f6c1ba6047b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
