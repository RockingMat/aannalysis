{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "# import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from minicons.utils import find_pattern\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['DT', 'ADJ', 'NUMERAL', 'NOUN', \"ADV\"]\n",
    "pos2cat = {\n",
    "    'DT': 'DT',\n",
    "    'JJ': 'ADJ',\n",
    "    'JJR': 'ADJ',\n",
    "    'JJS': 'ADJ',\n",
    "    'CD': 'NUMERAL',\n",
    "    'NNS': 'NOUN',\n",
    "    'NNPS': 'NOUN',\n",
    "    'RB': 'ADV',\n",
    "    'CC': 'CC',\n",
    "    'TO': 'TO'\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    return [t.text for t in tokenizer(string)]\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\"TODO: make read all\"\"\"\n",
    "    return [i.strip() for i in open(path, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "\n",
    "def write_lines(lst, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        for entry in lst:\n",
    "            f.write(f\"{entry}\\n\")\n",
    "\n",
    "\n",
    "AANN_REGEX = r\"\\b(DT)(?:(?:\\s(RB))*\\s(JJ|JJR|JJS)(?:\\s(CC))*)+(\\s(CD)(?:\\s(TO|CC)\\s(CD))*)(\\s(NNS|NNPS))+\"\n",
    "\n",
    "\n",
    "def detect_aann(postag):\n",
    "    return re.search(AANN_REGEX, postag)\n",
    "\n",
    "def aann_meta(token_seq, const_pattern):\n",
    "    form_elements = {\n",
    "        'DT': [],\n",
    "        'ADJ': [],\n",
    "        'NUMERAL': [],\n",
    "        'NOUN': [],\n",
    "        'ADV': []\n",
    "    }\n",
    "    for token, element in zip(token_seq, const_pattern.split()):\n",
    "        category = pos2cat[element]\n",
    "        if category in cats:\n",
    "            form_elements[category].append(token)\n",
    "\n",
    "    fe_strings = {k:\" & \".join(v) for k,v in form_elements.items()}\n",
    "\n",
    "    # return form_elements\n",
    "    return fe_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_subtitles counts: 287 pct: 5.275735294117647e-05\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "open_subtitles counts: 2797 pct: 0.0005141544117647058\n",
      "qed counts: 112 pct: 0.00011666666666666667\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "qed counts: 1623 pct: 0.001690625\n",
      "bnc_spoken counts: 93 pct: 0.00010954205531520432\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "bnc_spoken counts: 1072 pct: 0.0012626783150311723\n",
      "wikipedia counts: 237 pct: 0.0009294117647058824\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "wikipedia counts: 1994 pct: 0.007819607843137255\n",
      "gutenberg counts: 83 pct: 7.345132743362832e-05\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "gutenberg counts: 564 pct: 0.0004991150442477876\n",
      "aochildes counts: 4 pct: 5.2356774770317375e-06\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "aochildes counts: 30 pct: 3.926758107773803e-05\n",
      "simple_wikipedia counts: 156 pct: 0.00022722174803438622\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "simple_wikipedia counts: 1959 pct: 0.0028533807974318115\n",
      "children_stories counts: 21 pct: 0.0002700617283950617\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "children_stories counts: 110 pct: 0.0014146090534979425\n",
      "cbt counts: 29 pct: 0.00011004899077485874\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "cbt counts: 173 pct: 0.0006564991518638125\n",
      "switchboard counts: 16 pct: 9.892419933226165e-05\n",
      "\n",
      "\n",
      "With all determiners:\n",
      "\n",
      "switchboard counts: 139 pct: 0.0008594039816990231\n"
     ]
    }
   ],
   "source": [
    "all_aanns = []\n",
    "ALL_AANNS = []\n",
    "\n",
    "patterns = set()\n",
    "PATTERNS = set()\n",
    "\n",
    "full_aann_data = []\n",
    "FULL_AANN_DATA = []\n",
    "\n",
    "for file in glob.glob(\"/home/km55359/rawdata/babylm_data/postags_100M/*.train\"):\n",
    "    corpus = re.split(r\"(/|.train)\", file)[-3]\n",
    "    pos = read_file(file)\n",
    "    sents = read_file(f\"/home/km55359/rawdata/babylm_data/babylm_100M/{corpus}.train\")\n",
    "\n",
    "    aanns = []\n",
    "    for i, seq in enumerate(pos):\n",
    "        # if re.search(r'\\bDT JJ CD (NNS|NNPS)', seq):\n",
    "        searched = detect_aann(seq)\n",
    "        if searched:\n",
    "            aanns.append((i, seq, searched.span()))\n",
    "\n",
    "    if len(aanns) == 0:\n",
    "        print(f\"No (permissive) AANNs found in {corpus}\")\n",
    "        break\n",
    "\n",
    "    construction_forms = []\n",
    "    construction_ids = []\n",
    "\n",
    "    CONSTRUCTION_FORMS = []\n",
    "    CONSTRUCTION_IDS = []\n",
    "\n",
    "    for entry in aanns:\n",
    "        idx, pos_seq, span = entry\n",
    "        construction_pattern = pos_seq[span[0] : span[1]]\n",
    "        construction_pattern_span = find_pattern(\n",
    "            construction_pattern.split(), pos_seq.split()\n",
    "        )\n",
    "\n",
    "        tokens = tokenize(sents[idx])\n",
    "        if tokens == []:\n",
    "            pass\n",
    "        else:\n",
    "            extracted_token_seq = tokens[\n",
    "                construction_pattern_span[0] : construction_pattern_span[1]\n",
    "            ]\n",
    "            if extracted_token_seq[0].lower().startswith(\"-a\"):\n",
    "                extracted_token_seq[0] = (\n",
    "                    extracted_token_seq[0].replace(\"-a\", \"a\").replace(\"-A\", \"A\")\n",
    "                )\n",
    "            if extracted_token_seq[0].lower() in [\"a\", \"an\", \"-a\", \"another\"]:\n",
    "                construction_form = \" \".join(extracted_token_seq)\n",
    "\n",
    "                construction_forms.append(construction_form)\n",
    "                construction_ids.append(idx)\n",
    "\n",
    "                all_aanns.append(construction_form)\n",
    "\n",
    "                patterns.add(construction_pattern)\n",
    "\n",
    "                construction_elements = aann_meta(\n",
    "                    extracted_token_seq, construction_pattern\n",
    "                )\n",
    "                construction_elements[\"sentence\"] = sents[idx]\n",
    "                construction_elements[\"sentence_idx\"] = idx\n",
    "                construction_elements[\"pattern\"] = construction_pattern\n",
    "                construction_elements[\"source\"] = corpus\n",
    "                construction_elements[\"construction\"] = construction_form\n",
    "\n",
    "                full_aann_data.append(construction_elements)\n",
    "\n",
    "            # for all cases, regardless of the first token\n",
    "            CONSTRUCTION_FORM = \" \".join(extracted_token_seq)\n",
    "\n",
    "            CONSTRUCTION_FORMS.append(construction_form)\n",
    "            CONSTRUCTION_IDS.append(idx)\n",
    "\n",
    "            ALL_AANNS.append(CONSTRUCTION_FORM)\n",
    "\n",
    "            PATTERNS.add(construction_pattern)\n",
    "\n",
    "            CONSTRUCTION_ELEMENTS = aann_meta(extracted_token_seq, construction_pattern)\n",
    "            CONSTRUCTION_ELEMENTS[\"sentence\"] = sents[idx]\n",
    "            CONSTRUCTION_ELEMENTS[\"sentence_idx\"] = idx\n",
    "            CONSTRUCTION_ELEMENTS[\"pattern\"] = construction_pattern\n",
    "            CONSTRUCTION_ELEMENTS[\"source\"] = corpus\n",
    "            CONSTRUCTION_ELEMENTS[\"construction\"] = CONSTRUCTION_FORM\n",
    "\n",
    "            FULL_AANN_DATA.append(CONSTRUCTION_ELEMENTS)\n",
    "\n",
    "    if len(CONSTRUCTION_FORMS) == 0:\n",
    "        print(f\"No AANNs found in {corpus}\")\n",
    "        break\n",
    "\n",
    "    write_lines(\n",
    "        construction_ids,\n",
    "        f\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_ids_{corpus}_train.txt\",\n",
    "    )\n",
    "\n",
    "    write_lines(\n",
    "        construction_forms,\n",
    "        f\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_forms_{corpus}_train.txt\",\n",
    "    )\n",
    "\n",
    "    write_lines(\n",
    "        CONSTRUCTION_IDS,\n",
    "        f\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_all_det_ids_{corpus}_train.txt\",\n",
    "    )\n",
    "\n",
    "    write_lines(\n",
    "        CONSTRUCTION_FORMS,\n",
    "        f\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_all_det_forms_{corpus}_train.txt\",\n",
    "    )\n",
    "\n",
    "    cts = len(construction_forms)\n",
    "    frac = cts / len(sents)\n",
    "    print(f\"{corpus} counts: {cts} pct: {frac}\")\n",
    "\n",
    "    print(\"\\n\\nWith all determiners:\\n\")\n",
    "\n",
    "    cts = len(CONSTRUCTION_FORMS)\n",
    "    frac = cts / len(sents)\n",
    "    print(f\"{corpus} counts: {cts} pct: {frac}\")\n",
    "\n",
    "write_lines(\n",
    "    all_aanns, f\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_forms_all_train.txt\"\n",
    ")\n",
    "\n",
    "write_lines(\n",
    "    ALL_AANNS, f\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_all_det_forms_all_train.txt\"\n",
    ")\n",
    "\n",
    "unique_aanns = list(set(all_aanns))\n",
    "write_lines(\n",
    "    unique_aanns,\n",
    "    f\"/home/km55359/rawdata/babylm_data/babylm_100M/aann_forms_all-unique_train.txt\",\n",
    ")\n",
    "\n",
    "unique_aanns = list(set(ALL_AANNS))\n",
    "write_lines(\n",
    "    unique_aanns,\n",
    "    f\"/home/km55359/rawdata/babylm_data/babylm_100M/aann_all_det_forms_all-unique_train.txt\",\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"source\",\n",
    "    \"sentence\",\n",
    "    \"sentence_idx\",\n",
    "    \"construction\",\n",
    "    \"pattern\",\n",
    "    \"DT\",\n",
    "    \"ADJ\",\n",
    "    \"NUMERAL\",\n",
    "    \"NOUN\",\n",
    "    \"ADV\",\n",
    "]\n",
    "df = pd.DataFrame(full_aann_data)[cols]\n",
    "\n",
    "df.reset_index()\n",
    "df.to_csv(\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_data.csv\", index=False)\n",
    "df.to_csv(\"../data/baby_aann_data.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(FULL_AANN_DATA)[cols]\n",
    "\n",
    "df.reset_index()\n",
    "df.to_csv(\"/home/km55359/rawdata/babylm_data/babylm_100M/aanns/aann_all_det_data.csv\", index=False)\n",
    "df.to_csv(\"../data/babylm_aann_all_det_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DT': 'a',\n",
       "  'ADJ': 'few',\n",
       "  'NUMERAL': 'thousand',\n",
       "  'NOUN': 'dollars',\n",
       "  'ADV': '',\n",
       "  'sentence': \"We're talking a few thousand dollars!\",\n",
       "  'sentence_idx': 16438,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a few thousand dollars'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'measly',\n",
       "  'NUMERAL': 'thousand',\n",
       "  'NOUN': 'pounds',\n",
       "  'ADV': '',\n",
       "  'sentence': \"Mister, you wouldn't want to own a dog that couldn't pull a measly thousand pounds.\",\n",
       "  'sentence_idx': 50432,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a measly thousand pounds'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'fine',\n",
       "  'NUMERAL': 'eighteen',\n",
       "  'NOUN': 'months',\n",
       "  'ADV': '',\n",
       "  'sentence': 'I see .. all in all, a fine eighteen months.',\n",
       "  'sentence_idx': 53752,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a fine eighteen months'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'massive',\n",
       "  'NUMERAL': '2.5',\n",
       "  'NOUN': 'kilograms',\n",
       "  'ADV': '',\n",
       "  'sentence': \"And to blow up the bathtub, we're stretching the budget and moving into the realm of ridiculous by using a massive 2.5 kilograms of sodium.\",\n",
       "  'sentence_idx': 102201,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a massive 2.5 kilograms'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'good',\n",
       "  'NUMERAL': '12',\n",
       "  'NOUN': 'hours',\n",
       "  'ADV': '',\n",
       "  'sentence': 'We were filming for a good 12 hours or so.',\n",
       "  'sentence_idx': 134229,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a good 12 hours'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'few',\n",
       "  'NUMERAL': 'hundred',\n",
       "  'NOUN': 'survivors',\n",
       "  'ADV': '',\n",
       "  'sentence': 'Robert and a few hundred survivors dragged themselves west.',\n",
       "  'sentence_idx': 136016,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a few hundred survivors'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'few',\n",
       "  'NUMERAL': 'hundred',\n",
       "  'NOUN': 'men',\n",
       "  'ADV': '',\n",
       "  'sentence': 'He had no more than a few hundred men.',\n",
       "  'sentence_idx': 136049,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a few hundred men'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'fucking',\n",
       "  'NUMERAL': 'million',\n",
       "  'NOUN': 'years',\n",
       "  'ADV': '',\n",
       "  'sentence': 'Never in a fucking million years did I expect to hear that from him.',\n",
       "  'sentence_idx': 214659,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a fucking million years'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'staggering',\n",
       "  'NUMERAL': '22',\n",
       "  'NOUN': 'years',\n",
       "  'ADV': '',\n",
       "  'sentence': \"That's a staggering 22 years.\",\n",
       "  'sentence_idx': 216079,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a staggering 22 years'},\n",
       " {'DT': 'a',\n",
       "  'ADJ': 'staggering',\n",
       "  'NUMERAL': '22',\n",
       "  'NOUN': 'years',\n",
       "  'ADV': '',\n",
       "  'sentence': \"That's a staggering 22 years.\",\n",
       "  'sentence_idx': 217149,\n",
       "  'pattern': 'DT JJ CD NNS',\n",
       "  'source': 'open_subtitles',\n",
       "  'construction': 'a staggering 22 years'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_aann_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aann in full_aann_data:\n",
    "    if '-a' in aann['construction'].lower():\n",
    "        print(aann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031, 10461)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_aanns), len(ALL_AANNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = defaultdict(list)\n",
    "for const in all_aanns:\n",
    "    # const = const.lower()\n",
    "    article = const.split(\" \")[0]\n",
    "    articles[article].append(const)\n",
    "articles = dict(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'a', 'THE', 'The', 'these', 'those', 'an', 'An', 'each', 'any', 'that', 'A', 'another', 'That', 'this', 'THESE', 'These', 'Another', 'all', '-A', 'every', 'Those', 'This', 'THOSE', 'no', 'forthe', 'thesearethe', 'tbe', 'some', 'Every', 'All', \"'\", 'Each'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lens = {k: len(v) for k, v in articles.items()}\n",
    "sorted_articles = sorted(article_lens.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 8287),\n",
       " ('a', 785),\n",
       " ('The', 768),\n",
       " ('an', 185),\n",
       " ('these', 112),\n",
       " ('those', 64),\n",
       " ('THE', 45),\n",
       " ('A', 39),\n",
       " ('this', 31),\n",
       " ('These', 26),\n",
       " ('each', 23),\n",
       " ('An', 21),\n",
       " ('that', 18),\n",
       " ('any', 11),\n",
       " ('another', 6),\n",
       " ('every', 6),\n",
       " ('all', 5),\n",
       " ('Those', 5),\n",
       " ('This', 3),\n",
       " ('no', 3),\n",
       " ('forthe', 3),\n",
       " ('some', 3),\n",
       " ('THESE', 2),\n",
       " ('That', 1),\n",
       " ('Another', 1),\n",
       " ('-A', 1),\n",
       " ('THOSE', 1),\n",
       " ('thesearethe', 1),\n",
       " ('tbe', 1),\n",
       " ('Every', 1),\n",
       " ('All', 1),\n",
       " (\"'\", 1),\n",
       " ('Each', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this vulnerable 24 hours',\n",
       " 'this past six months',\n",
       " 'this first four courses',\n",
       " 'this negative 26.03 m',\n",
       " 'this first four bikes',\n",
       " 'this last four days',\n",
       " 'this last ten minutes',\n",
       " 'this 1st 5 years',\n",
       " 'this past 24 hours',\n",
       " 'this next one--0 times',\n",
       " 'this final two weeks',\n",
       " 'this first two instructions',\n",
       " 'this entire 42,000 miles',\n",
       " 'this past six months',\n",
       " 'this last fifteen years',\n",
       " 'this magic three hours',\n",
       " 'this first two years',\n",
       " 'this last three or four years',\n",
       " 'this last four weeks',\n",
       " 'this next six months',\n",
       " 'this last fifteen years',\n",
       " 'this extra few hundred pounds',\n",
       " 'this last two years',\n",
       " 'this last nine months',\n",
       " 'this last six months',\n",
       " 'this extra twenty hectares',\n",
       " 'this last three quarters',\n",
       " 'this past ten years',\n",
       " 'this last six months',\n",
       " 'this few hundred dollars',\n",
       " 'this last three years',\n",
       " 'this bad seventeen years',\n",
       " 'this last six months',\n",
       " 'this splendid six feet']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
