{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from minicons.utils import find_pattern\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import config\n",
    "import minicons.utils as mu\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "def tokenize(string):\n",
    "    return [t.text for t in tokenizer(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sents and postags\n",
    "sent_dir = \"/home/km55359/rawdata/babylm_data/babylm_100M/sents/\"\n",
    "# sent_dir = \"/Users/kanishka/rawdata/babylm-sents-and-postags/\"\n",
    "sents = utils.read_file(f\"{sent_dir}/babylm_sents.txt\")\n",
    "postags = utils.read_file(f\"{sent_dir}/postags.txt\")\n",
    "\n",
    "# sent_tokens = [tokenize(sent) for sent in sents]\n",
    "\n",
    "# # write sent tokens to file\n",
    "# with open(f\"{sent_dir}/sent_tokens.txt\", \"w\") as f:\n",
    "#     for sent in sent_tokens:\n",
    "#         f.write(\" \".join(sent) + \"\\n\")\n",
    "\n",
    "sent_tokens = utils.read_file(f\"{sent_dir}/sent_tokens.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMISSIVE = r'\\b(a|an|another)\\b .{0,100} \\b(two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion|several|few|couple|dozen|tens|dozens|hundreds|thousands|millions|billions|[0-9]+)\\b.{0,100}\\b\\w+s\\b'\n",
    "\n",
    "CURRENT = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC)\\s)+)?((JJ|JJR|JJS|VBN|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'\n",
    "\n",
    "CURRENT_DT = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|JJR|JJS|VBN|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?(DT\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'\n",
    "\n",
    "CURRENT_DEP_PARSE = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|NN|JJR|JJS|VBN|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numerals = ['few', 'dozen', 'couple', 'several', 'many', 'more']\n",
    "\n",
    "CURRENT_FEW = r'\\bARTICLE\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|JJR|JJS|VBN|RECORD|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN|RECORD)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?((JJR|JJ|VBN)\\s)?(ARTICLE\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'\n",
    "\n",
    "def detect_aann_basic(sent_toks, pos):\n",
    "    p_replaced = []\n",
    "    for st, pt in zip(sent_toks, pos.split(\" \")):\n",
    "        if st.lower() not in non_numerals + [\"a\", \"an\", \"another\", \"an\", \"-a\", \"-an\"] + ['record']:\n",
    "            p_replaced.append(pt)\n",
    "        else:\n",
    "            if st.lower() in [\"a\", \"another\", \"an\", \"-a\", \"-an\"]:\n",
    "                p_replaced.append(\"ARTICLE\")\n",
    "            elif st.lower() in non_numerals:\n",
    "                p_replaced.append(\"FEW\")\n",
    "            elif st.lower() in [\"record\"]:\n",
    "                p_replaced.append(\"RECORD\")\n",
    "            else:\n",
    "                p_replaced.append(pt)\n",
    "        # if st.lower() in non_numerals:\n",
    "        #     p_replaced.append(\"FEW\")\n",
    "        # else:\n",
    "        #     p_replaced.append(pt)\n",
    "        # if st.lower() in [\"a\", \"another\", \"an\", \"-a\", \"-an\"]:\n",
    "        #     p_replaced.append(\"ARTICLE\")\n",
    "        # else:\n",
    "        #     p_replaced.append(pt)\n",
    "    postag_seq = \" \".join(p_replaced)\n",
    "    print(postag_seq)\n",
    "    searched = re.search(CURRENT_FEW, postag_seq)\n",
    "    result = (False, \"\")\n",
    "    if searched:\n",
    "        span = searched.span()\n",
    "        construction_pattern = postag_seq[span[0] : span[1]]\n",
    "        construction_pattern_span = find_pattern(\n",
    "            construction_pattern.split(), postag_seq.split()\n",
    "        )\n",
    "        if sent_toks != []:\n",
    "            try:\n",
    "                sent_span = sent_toks[construction_pattern_span[0] : construction_pattern_span[1]]\n",
    "                result = (True, \" \".join(sent_span))\n",
    "            except:\n",
    "                pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few_pos = defaultdict(int)\n",
    "\n",
    "# for i, (s, sts, p) in enumerate(tqdm(zip(sents, sent_tokens, postags))):\n",
    "#     # align s to p\n",
    "    \n",
    "# permissed_sampled_annotated = utils.read_csv_dict(\"../data/permissed_sampled_annotated_old1k.csv\")\n",
    "permissed_sampled_annotated = utils.read_csv_dict(\"../data/permissed_sampled_annotated_test1_1k_predictions.csv\")\n",
    "\n",
    "# with open(\"../data/permissed_sampled_annotated_old1k_new_regex.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"sentence\",\"pos\",\"span\",\"length\",\"aann\",\"og_detected\",\"current_detected\",\"dep_parse_detected\",\"current_with_dt_detected\", \"final_detected\"])\n",
    "#     for i, entry in enumerate(permissed_sampled_annotated):\n",
    "#         sent_tok = tokenize(entry['sentence'])\n",
    "#         detected, span = detect_aann_basic(sent_tok, entry['pos'])\n",
    "#         writer.writerow([entry['sentence'], entry['pos'], entry['span'], entry['length'], entry['aann'], entry['og_detected'], entry['current_detected'], entry['dep_parse_detected'], entry['current_with_dt_detected'], int(detected)])\n",
    "\n",
    "\n",
    "with open(\"../data/permissed_sampled_annotated_test1_1k_final_regex.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence\",\"pos\",\"span\",\"length\",\"aann\",\"og_detected\",\"current_detected\",\"dep_parse_detected\",\"current_with_dt_detected\", \"final_detected\"])\n",
    "    for i, entry in enumerate(permissed_sampled_annotated):\n",
    "        sent_tok = tokenize(entry['sentence'])\n",
    "        detected, span = detect_aann_basic(sent_tok, entry['pos'])\n",
    "        writer.writerow([entry['sentence'], entry['pos'], entry['span'], entry['length'], entry['aann'], entry['og_detected'], entry['current_detected'], entry['dep_parse_detected'], entry['current_with_dt_detected'], int(detected)])\n",
    "\n",
    "# for i, entry in enumerate(permissed_sampled_annotated):\n",
    "            \n",
    "#         # if detected :\n",
    "#             # print(span, entry['aann'])\n",
    "#     sent_tok = tokenize(entry['sentence'])\n",
    "#     detected, span = detect_aann_basic(sent_tok, entry['postags'])\n",
    "#     if entry['aann'] == '1':\n",
    "#         if detected:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(i, entry['span'], entry['postags'])\n",
    "#         # if entry['aann'] == '1':\n",
    "#         #     if \"few\" in entry['sentence']:\n",
    "#         #         print(i, entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'Buoyed by a record 1.5 million pre-orders, the game sold 2.4 million copies in a single day, for a total of 125 million dollars.', 'pos': 'VBN IN DT NN CD CD NNS NNS NNS , DT NN VBD CD CD NNS IN DT JJ NN , IN DT NN IN CD CD NNS .', 'span': 'a record 1.5 million pre-orders, the game sold 2.4 million copies in a single day, for a total of 125 million dollars', 'length': '117', 'aann': '1', 'og_detected': '0', 'current_detected': '0', 'dep_parse_detected': '1', 'current_with_dt_detected': '0'}\n",
      "VBN IN ARTICLE RECORD CD CD NNS NNS NNS , DT NN VBD CD CD NNS IN ARTICLE JJ NN , IN ARTICLE NN IN CD CD NNS .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'a record 1.5 million pre')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# few_pos\n",
    "print(permissed_sampled_annotated[290])\n",
    "\n",
    "sent_toks = tokenize(permissed_sampled_annotated[290]['sentence'])\n",
    "detect_aann_basic(sent_toks, permissed_sampled_annotated[290]['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_record = []\n",
    "for i, (s, p)  in enumerate(zip(sents, postags)):\n",
    "    if \"club record\" in s or \"club-record\" in s:\n",
    "        club_record.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN DT CD NN , NNP VBD ARTICLE NN HYPH NN RECORD IN CD , VBD RP CD NNS , CC VBD ARTICLE JJ CD VBN NN NN , IN NNP VBD ARTICLE NN HYPH RECORD CD NNS CC DT RB JJ NNP NNP NNP NNP NN -LRB- RB IN DT NN , CC DT NNP NNP CC NNP NNP VBD VBN VBN IN NNP CC NNP NNPS -RRB- -RRB-\n",
      "(True, 'a club - record 109 games')\n",
      "DT NN VBD VBN IN NNP POS NNP NNP IN NN IN ARTICLE NN HYPH RECORD NN IN CD .\n",
      "(False, '')\n",
      "DT NN VBD PRP$ NNP NNP RECORD IN CD NN NNS IN CD NNS , CC VBD TO VB ARTICLE JJ JJ NN DT NN , VBG ARTICLE NN RECORD NN IN CD NNS IN ARTICLE NN WDT VBD RB DT JJ NN CC VBD IN NNP CD .\n",
      "(False, '')\n",
      "IN ARTICLE CD NN IN NNP NNP , DT NNPS VBD IN ARTICLE JJ NN IN DT NN IN DT NNP NNP , VBG DT NN IN ARTICLE NN IN ARTICLE NN RECORD JJ NN VBG .\n",
      "(False, '')\n",
      "IN JJ NN NNP NNP VBD VBN IN NNP NNP NN NNP IN ARTICLE JJ NNP NN RECORD NN IN $ CD IN NNP CD , DT NNPS VBD TO VB IN DT JJ CD NNS IN DT NN , VBG RB RB IN JJ IN ARTICLE NN IN CD NNS IN RB CD NN .\n",
      "(False, '')\n",
      "CD FEW NN NNS IN NNP : IN NNP , NNP NNP NNP CC NNP NNP : VBD IN DT NN VBD NN IN DT NNP NNP NNP : NNP VBD RB IN DT NN NN IN CD NNS -LRB- DT IN NN , VBG DT NN RECORD , , CD NNS CC CD NNS .\n",
      "(False, '')\n",
      "IN VBG IN NNP , PRP VBD DT NN IN ARTICLE NN RECORD CD NN NNS JJ , IN PRP$ JJ CD NN IN DT NN NN IN NNP .\n",
      "(False, '')\n",
      "PRP$ CD HYPH NN NN VBD DT JJ NN NN NN IN PRP$ NN , VBG NNP NNP IN DT NN RECORD .\n",
      "(False, '')\n",
      "PRP VBD NNP NNP POS NN RECORD IN NNS IN ARTICLE NN IN ARTICLE NN .\n",
      "(False, '')\n",
      "PRP VBD PRP$ JJ NN IN JJ NN RB IN NN IN CD NNS CC DT NN VBD ARTICLE JJ NN RECORD , IN NNP VBD DT JJS NN IN DT NN IN CD NNS .\n",
      "(False, '')\n",
      "PRP VBD CD NNS IN DT NN IN NNP VBD ARTICLE NN RECORD CD JJ NNS : RB PRP MD RB VB DT NN HYPH NN NNS NNS NNS .\n",
      "(False, '')\n",
      "RB , IN DT NN IN NNP NNP , NNP NNP VBZ DT NN RECORD IN JJS NN NN IN CD CD .\n",
      "(False, '')\n",
      "IN NNP CD , NNP VBD DT JJ JJ NN NNP NNP IN ARTICLE NN RECORD NN IN CD CD NNS WDT VBD DT NN POS JJ NNS CD IN DT JJS IN DT NN .\n",
      "(False, '')\n",
      "PRP VBZ IN DT NN RECORD IN JJ NNS IN ARTICLE NN -LRB- CD -RRB- CC PRP$ NN VBD RB VBN IN ARTICLE VBN NN VBN IN CD .\n",
      "(False, '')\n",
      "IN PRP$ NN PRP VBD ARTICLE NN IN NNP NNP , RB TO VB PRP$ JJ NN IN CD NNP CD IN ARTICLE NN RECORD NN IN CD NNS CC CD NNS .\n",
      "(False, '')\n",
      "PRP VBD NNS NNP NNP CC NNP NNP , CC VBD DT NNP IN ARTICLE NN RECORD CD NN IN NNP IN CD NNP .\n",
      "(False, '')\n",
      "IN CD NNP CD , NNP NNP VBD IN PRP VBD VBN NNP IN NNP IN ARTICLE NN RECORD $ CD CD NN IN ARTICLE CD HYPH NN NN .\n",
      "(False, '')\n",
      "NNP NNP VBD ARTICLE NN RECORD VBG NN IN CD NNS VBG DT NN POS CC NN POS NNP IN DT NNP .\n",
      "(False, '')\n",
      "PRP VBZ ARTICLE NN RECORD IN DT JJ NN IN NNS NNP CC NNP NNP VBP CC VBP .\n",
      "(False, '')\n",
      "NNP NNP POS JJ NN IN DT NN VBD ARTICLE JJ NN RECORD IN JJS RB NN IN DT ARTICLE HYPH NNP .\n",
      "(False, '')\n",
      "DT NN VBD NNP NNP POS JJ NN RECORD IN CD NNS , VBN WRB DT NN VBD RB VBN NNP NNP RB IN DT CD NNP NNP NNP NN .\n",
      "(False, '')\n",
      "IN ARTICLE JJ NN NN RB DT NN , DT NN VBD VBN IN ARTICLE JJ NN IN DT NN NNS IN DT JJ NNP NNP , VBN IN CD .\n",
      "(False, '')\n",
      "DT JJ NN VBD VBN NNP NNP NNP CC VBD RB VBN IN ARTICLE NN IN DT NN NNS IN DT JJ NNP NNP , WDT VBD IN FEW IN CD NNS IN DT JJ NN .\n",
      "(False, '')\n",
      "PRP VBD JJ NN IN DT NNP NNP IN DT CD NN IN CD NNS , CC -LRB- IN IN CD -RRB- VBD DT NN RECORD IN JJS NNS IN ARTICLE JJ NN , IN CD IN DT NNPS IN ARTICLE CD NN IN NNP NNPS .\n",
      "(False, '')\n",
      "NNP VBD NNP NNP NNS NNP IN NNP CD , IN WP VBD RB ARTICLE NN HYPH RECORD NN IN ARTICLE NN , $ CD .\n",
      "(False, '')\n",
      "IN CD NNS CC ARTICLE NN IN CD NN HYPH NNS , NNP VBD ARTICLE NN RECORD IN DT JJS NN NN IN NN , RB CD NNS IN DT NN IN DT NN .\n",
      "(False, '')\n",
      "IN DT CD NN VBD DT NN RECORD ARTICLE RECORD NN : ARTICLE CD NN IN NNP NNP .\n",
      "(False, '')\n",
      "DT VBD DT NN RECORD .\n",
      "(False, '')\n",
      "PRP VBD CD IN NNP , ARTICLE NN RECORD .\n",
      "(False, '')\n",
      "PRP VBD ARTICLE NN RECORD IN VBG IN CD JJ NNS .\n",
      "(False, '')\n"
     ]
    }
   ],
   "source": [
    "for i in club_record:\n",
    "    st, pt = sent_tokens[i], postags[i]\n",
    "    print(detect_aann_basic(st, pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_detected = utils.read_csv_dict(\"../data/babylm-aanns/aanns_indef_all.csv\")\n",
    "old_idx = [int(entry['sentence_idx']) for entry in old_detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11632617it [00:49, 235227.64it/s]\n"
     ]
    }
   ],
   "source": [
    "new_idx = []\n",
    "new_entries = []\n",
    "\n",
    "for i, (s, st, p) in enumerate(tqdm(zip(sents, sent_tokens, postags))):\n",
    "    # if i in old_idx:\n",
    "    #     continue\n",
    "    detected, span = detect_aann_basic(st, p)\n",
    "    if detected:\n",
    "        new_idx.append(i)\n",
    "        new_entries.append((i, span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2448"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1349"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(old_idx).intersection(set(new_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_idx_few = []\n",
    "for entry in old_detected:\n",
    "    if \"few\" in entry['ADJ'] or \"couple\" in entry['ADJ'] or \"several\" in entry['ADJ'] or \"many\" in entry['ADJ'] or \"more\" in entry['ADJ'] or 'dozen' in entry['ADJ']:\n",
    "        old_idx_few.append(int(entry['sentence_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_idx_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{410230,\n",
       " 463226,\n",
       " 611182,\n",
       " 1064925,\n",
       " 1662035,\n",
       " 1925531,\n",
       " 2278535,\n",
       " 2695635,\n",
       " 2696537,\n",
       " 4023499,\n",
       " 4251011}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len((set(old_idx) - set(old_idx_few)).intersection(set(new_idx)))\n",
    "(set(old_idx) - set(old_idx_few)) - set(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save old union new ids\n",
    "old_union_new = list(set(old_idx).union(set(new_idx)))\n",
    "\n",
    "with open(\"../data/babylm-analysis/old_union_new_regex_aanns.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence_idx\"])\n",
    "    for idx in old_union_new:\n",
    "        writer.writerow([idx])\n",
    "\n",
    "# save new ids alone\n",
    "\n",
    "with open(\"../data/babylm-analysis/new_regex_aanns.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence_idx\"])\n",
    "    for idx in new_idx:\n",
    "        writer.writerow([idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmisra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
