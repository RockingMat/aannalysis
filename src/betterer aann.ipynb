{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from minicons.utils import find_pattern\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import config\n",
    "import minicons.utils as mu\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from minicons import utils as mu\n",
    "import inflect\n",
    "from constructions import AANN\n",
    "\n",
    "inflector = inflect.engine()\n",
    "import editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "def tokenize(string):\n",
    "    return [t.text for t in tokenizer(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sents and postags\n",
    "# sent_dir = \"/home/km55359/rawdata/babylm_data/babylm_100M/sents/\"\n",
    "# sent_dir = \"/Users/kanishka/rawdata/babylm-sents-and-postags/\"\n",
    "sent_dir = \"../data/babylm/\"\n",
    "sents = utils.read_file(f\"{sent_dir}/train.sents\")\n",
    "postags = utils.read_file(f\"{sent_dir}/train.postags\")\n",
    "\n",
    "sent_tokens = [tokenize(sent) for sent in sents]\n",
    "\n",
    "# # write sent tokens to file\n",
    "# with open(f\"{sent_dir}/sent_tokens.txt\", \"w\") as f:\n",
    "#     for sent in sent_tokens:\n",
    "#         f.write(\" \".join(sent) + \"\\n\")\n",
    "\n",
    "# sent_tokens = utils.read_file(f\"{sent_dir}/sent_tokens.txt\")\n",
    "# sent_tokens = [sent.strip().split(\" \") for sent in sent_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMISSIVE = r'\\b(a|an|another)\\b .{0,100} \\b(two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion|several|few|couple|dozen|tens|dozens|hundreds|thousands|millions|billions|[0-9]+)\\b.{0,100}\\b\\w+s\\b'\n",
    "\n",
    "CURRENT = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC)\\s)+)?((JJ|JJR|JJS|VBN|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'\n",
    "\n",
    "CURRENT_DT = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|JJR|JJS|VBN|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?(DT\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'\n",
    "\n",
    "CURRENT_DEP_PARSE = r'\\bDT\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|NN|JJR|JJS|VBN|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?(JJR\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numerals = ['few', 'dozen', 'couple', 'several', 'many', 'more']\n",
    "\n",
    "CURRENT_FEW = r'\\bARTICLE\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|JJR|JJS|VBN|RECORD|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN|RECORD)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?((JJR|JJ|VBN)\\s)?(ARTICLE\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'\n",
    "\n",
    "def detect_aann_basic(sent_toks, pos):\n",
    "    p_replaced = []\n",
    "    for st, pt in zip(sent_toks, pos.split(\" \")):\n",
    "        if st.lower() not in non_numerals + [\"a\", \"an\", \"another\", \"an\", \"-a\", \"-an\"] + ['record']:\n",
    "            p_replaced.append(pt)\n",
    "        else:\n",
    "            if st.lower() in [\"a\", \"another\", \"an\", \"-a\", \"-an\"]:\n",
    "                p_replaced.append(\"ARTICLE\")\n",
    "            elif st.lower() in non_numerals:\n",
    "                p_replaced.append(\"FEW\")\n",
    "            elif st.lower() in [\"record\"]:\n",
    "                p_replaced.append(\"RECORD\")\n",
    "            else:\n",
    "                p_replaced.append(pt)\n",
    "        # if st.lower() in non_numerals:\n",
    "        #     p_replaced.append(\"FEW\")\n",
    "        # else:\n",
    "        #     p_replaced.append(pt)\n",
    "        # if st.lower() in [\"a\", \"another\", \"an\", \"-a\", \"-an\"]:\n",
    "        #     p_replaced.append(\"ARTICLE\")\n",
    "        # else:\n",
    "        #     p_replaced.append(pt)\n",
    "    postag_seq = \" \".join(p_replaced)\n",
    "    # print(postag_seq)\n",
    "    searched = re.search(CURRENT_FEW, postag_seq)\n",
    "    result = (False, \"\", \"\")\n",
    "    if searched:\n",
    "        span = searched.span()\n",
    "        construction_pattern = postag_seq[span[0] : span[1]]\n",
    "        construction_pattern_span = find_pattern(\n",
    "            construction_pattern.split(), postag_seq.split()\n",
    "        )\n",
    "        if sent_toks != []:\n",
    "            try:\n",
    "                sent_span = sent_toks[construction_pattern_span[0] : construction_pattern_span[1]]\n",
    "                result = (True, \" \".join(sent_span), construction_pattern)\n",
    "            except:\n",
    "                pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALL ANALYSIS DATA\n",
    "\n",
    "# few_pos = defaultdict(int)\n",
    "\n",
    "# for i, (s, sts, p) in enumerate(tqdm(zip(sents, sent_tokens, postags))):\n",
    "#     # align s to p\n",
    "    \n",
    "# permissed_sampled_annotated = utils.read_csv_dict(\"../data/permissed_sampled_annotated_old1k.csv\")\n",
    "permissed_sampled_annotated = utils.read_csv_dict(\"../data/permissed_sampled_annotated_test1_1k_predictions.csv\")\n",
    "\n",
    "# with open(\"../data/permissed_sampled_annotated_old1k_new_regex.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"sentence\",\"pos\",\"span\",\"length\",\"aann\",\"og_detected\",\"current_detected\",\"dep_parse_detected\",\"current_with_dt_detected\", \"final_detected\"])\n",
    "#     for i, entry in enumerate(permissed_sampled_annotated):\n",
    "#         sent_tok = tokenize(entry['sentence'])\n",
    "#         detected, span = detect_aann_basic(sent_tok, entry['pos'])\n",
    "#         writer.writerow([entry['sentence'], entry['pos'], entry['span'], entry['length'], entry['aann'], entry['og_detected'], entry['current_detected'], entry['dep_parse_detected'], entry['current_with_dt_detected'], int(detected)])\n",
    "\n",
    "\n",
    "with open(\"../data/permissed_sampled_annotated_test1_1k_final_regex.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence\",\"pos\",\"span\",\"length\",\"aann\",\"og_detected\",\"current_detected\",\"dep_parse_detected\",\"current_with_dt_detected\", \"final_detected\"])\n",
    "    for i, entry in enumerate(permissed_sampled_annotated):\n",
    "        sent_tok = tokenize(entry['sentence'])\n",
    "        detected, span = detect_aann_basic(sent_tok, entry['pos'])\n",
    "        writer.writerow([entry['sentence'], entry['pos'], entry['span'], entry['length'], entry['aann'], entry['og_detected'], entry['current_detected'], entry['dep_parse_detected'], entry['current_with_dt_detected'], int(detected)])\n",
    "\n",
    "# for i, entry in enumerate(permissed_sampled_annotated):\n",
    "            \n",
    "#         # if detected :\n",
    "#             # print(span, entry['aann'])\n",
    "#     sent_tok = tokenize(entry['sentence'])\n",
    "#     detected, span = detect_aann_basic(sent_tok, entry['postags'])\n",
    "#     if entry['aann'] == '1':\n",
    "#         if detected:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(i, entry['span'], entry['postags'])\n",
    "#         # if entry['aann'] == '1':\n",
    "#         #     if \"few\" in entry['sentence']:\n",
    "#         #         print(i, entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few_pos\n",
    "print(permissed_sampled_annotated[290])\n",
    "\n",
    "sent_toks = tokenize(permissed_sampled_annotated[290]['sentence'])\n",
    "detect_aann_basic(sent_toks, permissed_sampled_annotated[290]['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_record = []\n",
    "for i, (s, p)  in enumerate(zip(sents, postags)):\n",
    "    if \"club record\" in s or \"club-record\" in s:\n",
    "        club_record.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in club_record:\n",
    "    st, pt = sent_tokens[i], postags[i]\n",
    "    print(detect_aann_basic(st, pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_detected = utils.read_csv_dict(\"../data/babylm-aanns/aanns_indef_all.csv\")\n",
    "old_idx = [int(entry['sentence_idx']) for entry in old_detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_idx = []\n",
    "new_entries = []\n",
    "\n",
    "for i, (s, st, p) in enumerate(tqdm(zip(sents, sent_tokens, postags))):\n",
    "    # if i in old_idx:\n",
    "    #     continue\n",
    "    detected, span = detect_aann_basic(st, p)\n",
    "    if detected:\n",
    "        new_idx.append(i)\n",
    "        new_entries.append((i, span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(old_idx).intersection(set(new_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_idx_few = []\n",
    "for entry in old_detected:\n",
    "    if \"few\" in entry['ADJ'] or \"couple\" in entry['ADJ'] or \"several\" in entry['ADJ'] or \"many\" in entry['ADJ'] or \"more\" in entry['ADJ'] or 'dozen' in entry['ADJ']:\n",
    "        old_idx_few.append(int(entry['sentence_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_idx_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len((set(old_idx) - set(old_idx_few)).intersection(set(new_idx)))\n",
    "(set(old_idx) - set(old_idx_few)) - set(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save old union new ids\n",
    "old_union_new = list(set(old_idx).union(set(new_idx)))\n",
    "\n",
    "with open(\"../data/babylm-analysis/old_union_new_regex_aanns.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence_idx\"])\n",
    "    for idx in old_union_new:\n",
    "        writer.writerow([idx])\n",
    "\n",
    "# save new ids alone\n",
    "\n",
    "with open(\"../data/babylm-analysis/new_regex_aanns.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence_idx\"])\n",
    "    for idx in new_idx:\n",
    "        writer.writerow([idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\bARTICLE\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|JJR|JJS|VBN|RECORD|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN|RECORD)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?((JJR|JJ|VBN)\\s)?(ARTICLE\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FEW = r'\\bARTICLE\\s(((HYPH|,)\\s))?((((RB|CC|IN)\\s)+)?((JJ|JJR|JJS|VBN|RECORD|((NN CC NN |NN HYPH )+(JJ|JJR|JJS|VBN|RECORD)))((\\s(HYPH|,))?)\\s))+(((RB)\\s)+)?(((HYPH|,)\\s))?((UH)\\s)?(((NN|CC)\\s)+)?((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?((JJR|JJ|VBN)\\s)?(ARTICLE\\s)?((NNS|NNPS|(NN\\sNNS)|((NN|NNS) IN NNS)))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing\n",
    "\n",
    "cats = ['DT', 'ADJ', 'NUMERAL', 'NOUN', \"ADV\"]\n",
    "non_numerals = ['few', 'dozen', 'couple', 'several', 'many', 'more']\n",
    "\n",
    "pos2cat = {\n",
    "    'DT': 'DT',\n",
    "    'JJ': 'ADJ',\n",
    "    'JJR': 'ADJ',\n",
    "    'JJS': 'ADJ',\n",
    "    'CD': 'NUMERAL',\n",
    "    'NNS': 'NOUN',\n",
    "    'NNPS': 'NOUN',\n",
    "    'NN': 'NOUN',\n",
    "    'IN': 'NOUN',\n",
    "    'RB': 'ADV',\n",
    "    'CC': 'CC',\n",
    "    'TO': 'TO'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_entries = defaultdict(tuple)\n",
    "new_idx = []\n",
    "\n",
    "for i, (s, st, p) in enumerate(tqdm(zip(sents, sent_tokens, postags))):\n",
    "    # if i in old_idx:\n",
    "    #     continue\n",
    "    detected, span, pos_span = detect_aann_basic(st, p)\n",
    "    if detected:\n",
    "        new_entries[i] = (i, span, pos_span, st)\n",
    "        new_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naan(aann):\n",
    "    \"\"\"reverses the AANN order\"\"\"\n",
    "    if aann.article.lower() in [\"a\", \"an\"]:\n",
    "        try:\n",
    "            article = inflector.a(aann.noun.split(\" \")[0]).split(\" \")[0]\n",
    "        except:\n",
    "            article = aann.article\n",
    "    else:\n",
    "        article = aann.article\n",
    "    return AANN(aann.numeral, aann.adjective, article, aann.noun)\n",
    "\n",
    "def anan(aann):\n",
    "    \"\"\"converts AANN --> ANAN\"\"\"\n",
    "    if aann.article.lower() in [\"a\", \"an\"]:\n",
    "        try:\n",
    "            article = inflector.a(\n",
    "                f\"{aann.numeral} {aann.adjective}\".strip().split(\" \")[0]\n",
    "            ).split(\" \")[0]\n",
    "        except:\n",
    "            article = aann.article\n",
    "    else:\n",
    "        article = aann.article\n",
    "    return AANN(article, aann.numeral, aann.adjective, aann.noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11632617/11632617 [00:20<00:00, 563928.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# sents_with_naans = []\n",
    "# sents_with_anans = []\n",
    "\n",
    "sents_with_300_naans = []\n",
    "sents_with_300_anans = []\n",
    "\n",
    "random_new_idx = random.sample(new_idx, 300)\n",
    "\n",
    "\n",
    "for idx, s in enumerate(tqdm(sents)):\n",
    "    if idx in random_new_idx:\n",
    "        _, span, pos_span, st = new_entries[idx]\n",
    "        article_span = re.search(r\"ARTICLE\\s(((HYPH|,)\\s))?\", pos_span).group(0)\n",
    "        article_idx = mu.find_pattern(article_span.split(), pos_span.split())\n",
    "        article_phrase = \" \".join(span.split()[article_idx[0]:article_idx[1]])\n",
    "\n",
    "        num_span = re.search(r\"((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?\", pos_span).group(0)\n",
    "        num_idx = mu.find_pattern(num_span.split(), pos_span.split())\n",
    "        num_phrase = \" \".join(span.split()[num_idx[0]:num_idx[1]])\n",
    "\n",
    "        adj_idx = article_idx[-1], num_idx[0]\n",
    "        adj_phrase = \" \".join(span.split()[adj_idx[0]:adj_idx[1]])\n",
    "\n",
    "        noun_phrase = \" \".join(span.split()[num_idx[1]:])\n",
    "\n",
    "        aann = AANN(article_phrase, adj_phrase, num_phrase, noun_phrase)\n",
    "        naan_string = naan(aann).string\n",
    "        naan_replacement = \" \".join(st).replace(span, naan_string)\n",
    "        anan_string = anan(aann).string\n",
    "        anan_replacement = \" \".join(st).replace(span, anan_string)\n",
    "        sents_with_300_naans.append(naan_replacement)\n",
    "        sents_with_300_anans.append(anan_replacement)\n",
    "        # sents_with_naans.append(naan_replacement)\n",
    "        # sents_with_anans.append(anan_replacement)\n",
    "    else:\n",
    "        sents_with_300_naans.append(s)\n",
    "        sents_with_300_anans.append(s)\n",
    "        # sents_with_naans.append(s)\n",
    "        # sents_with_anans.append(s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for idx, entry in new_entries.items():\n",
    "#     idx, span, pos_span, st = entry\n",
    "    \n",
    "#     article_span = re.search(r\"ARTICLE\\s(((HYPH|,)\\s))?\", pos_span).group(0)\n",
    "#     article_idx = mu.find_pattern(article_span.split(), pos_span.split())\n",
    "#     article_phrase = \" \".join(span.split()[article_idx[0]:article_idx[1]])\n",
    "#     articles.append((idx, article_phrase))\n",
    "\n",
    "#     num_span = re.search(r\"((CD|FEW)(\\s(TO|CC|(HYPH|,))(\\s(HYPH|,))?)?\\s)+(((HYPH|,)\\s))?\", pos_span).group(0)\n",
    "#     num_idx = mu.find_pattern(num_span.split(), pos_span.split())\n",
    "#     num_phrase = \" \".join(span.split()[num_idx[0]:num_idx[1]])\n",
    "\n",
    "#     adj_idx = article_idx[-1], num_idx[0]\n",
    "#     adj_phrase = \" \".join(span.split()[adj_idx[0]:adj_idx[1]])\n",
    "\n",
    "#     noun_phrase = \" \".join(span.split()[num_idx[1]:])\n",
    "\n",
    "#     aann = AANN(article_phrase, adj_phrase, num_phrase, noun_phrase)\n",
    "#     # try:\n",
    "#     naan_rep = naan(aann)\n",
    "#     # except:\n",
    "#     #     print(aann, naan(aann), span, pos_span)\n",
    "#     aann_naans.append((span, naan_rep.string))\n",
    "#     # nums.append((i, num_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11632617"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(sents_with_naans), len(sents_with_anans)\n",
    "# len(sents_with_200_anans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/training_data/counterfactual_babylm_naans_new.txt\", \"w\") as f:\n",
    "#     for s in sents_with_naans:\n",
    "#         f.write(s + \"\\n\")\n",
    "\n",
    "# with open(\"../data/training_data/counterfactual_babylm_anans_new.txt\", \"w\") as f:\n",
    "#     for s in sents_with_anans:\n",
    "#         f.write(s + \"\\n\")\n",
    "\n",
    "with open(\"../data/training_data/counterfactual_babylm_300_naans_new.txt\", \"w\") as f:\n",
    "    for s in sents_with_300_naans:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "with open(\"../data/training_data/counterfactual_babylm_300_anans_new.txt\", \"w\") as f:\n",
    "    for s in sents_with_300_anans:\n",
    "        f.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921d2ca1710f419ead20bfb9663332b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc3babfcb3a4ec18b50c8877094a676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da1a164e76a4559960c28de52573c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed505cbeeb9548eb8839325d05f79505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5817 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4613c33080d44be96235bb77a5868cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5817 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76ed4ffa3b14ef182e99f17b28b11d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fdcf8536974bfd85d46eacc81450af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1027 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/kanishka/counterfactual_babylm_300_anans_new/commit/4c451d528429eadbf7e6b1de239ce8dbdd7fa07a', commit_message='Upload dataset', commit_description='', oid='4c451d528429eadbf7e6b1de239ce8dbdd7fa07a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_path = \"../data/training_data/counterfactual_babylm_naans_new.txt\"\n",
    "# output_path = \"../data/training_data/counterfactual_babylm_anans_new.txt\"\n",
    "# output_path = \"../data/training_data/counterfactual_babylm_300_naans_new.txt\"\n",
    "output_path = \"../data/training_data/counterfactual_babylm_300_anans_new.txt\"\n",
    "\n",
    "VAL_FILE = \"../../rawdata/babylm_data/babylm_dev/babylm_dev.txt\"\n",
    "\n",
    "data_files = {}\n",
    "dataset_args = {}\n",
    "data_files[\"train\"] = output_path,\n",
    "data_files[\"validation\"] = VAL_FILE\n",
    "dataset_args[\"keep_linebreaks\"] = True\n",
    "raw_datasets = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": output_path,\n",
    "        \"validation\": VAL_FILE\n",
    "    },\n",
    "    # token=model_args.token,\n",
    "    keep_linebreaks=True\n",
    ")\n",
    "\n",
    "raw_datasets.push_to_hub(f\"kanishka/{output_path.split('/')[-1].replace('.txt', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmisra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
